
---
# T4 - GPUs y CUDA

[Atrás](../README.md)

---
## Arquitectura
### Modelos de CPU
- **Modelo secuencial / tradicional**
	- Se ejecuta paso a paso.
- **SIMT - Single Instruction, Multiple Threads**
	- Se divide el trabajo en varios hilos (pocos hilos)
- **SIMD - Single Instruction , Multiple Data**
	- Se opera con vectores pequeños en un solo ciclo.
### Modelo de GPU
#### Física
1. **GPU - Graphics Processing Unit**
	- Dispone de una memoria global (VRAM)
2. **GPG - Graphics Processing Cluster**
3. **TPC - Texture Processing Cluster**
4. **SMs - Streaming Multiprocessors**
	- Tiene una memoria compartida más rapida (Shared Memory).
	- Dispone de un warp scheduler.
5. **Cuda Cores**
	- Tiene una memoria más rapida (Register File).
#### Lógica
1. Device (GPU)
	- Se refiere a la GPU.
	- Ejecuta los kernels.
2. Malla de bloques (Grid)
	- Representa el problema en su totalidad.
	- Es el conjunto de bloques definido al lanzar el kernel.
3. Bloque de hilos (Thread Block)
	- Unidad programable más pequeña.
	- Conjunto de hilos (max. 1024) que se define al lanzar un kernel.
	- Pueden trabajar en 1D, 2D y 3D. (Cuidado con el max. hilos)
	- Los bloques se asignan a un SM para que los ejecute.
4. Warp
	- Unidad ejecutable más pequeña.
	- Agrupación de 32 hilos gestinados por el Warp Scheduler.
	- Todos sus hilos se ejecutan de manera sincronizada.
	- No todos los hilos de un warp ejecutan instrucciones.
5. Hilo
	- El hilo es la unidad más pequeña de ejecución.
	- Cada hilo se ejecutará en un núcleo CUDA

---
