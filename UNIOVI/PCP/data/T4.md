
---
# T4 - GPUs y CUDA

[Atrás](../README.md)

---
## Arquitectura
### Modelos de CPU
- **Modelo secuencial / tradicional**
	- Se ejecuta paso a paso.
- **SIMT - Single Instruction, Multiple Threads**
	- Se divide el trabajo en varios hilos (pocos hilos)
- **SIMD - Single Instruction , Multiple Data**
	- Se opera con vectores pequeños en un solo ciclo.
### Modelo de GPU
#### Física
1. GPU - Graphics Processing Unit
	- Dispone de una memoria global (VRAM)
2. GPG - Graphics Processing Cluster
3. TPC - Texture Processing Cluster
4. SMs - Streaming Multiprocessors
	- Tiene una memoria compartida más rapida (Shared Memory).
	- Dispone de un warp scheduler.
1. Cuda Cores
	- 
Se estructuran de más pequeño a más grande:
1. **Hilo - Núcleo CUDA**
	- El hilo es la unidad más pequeña de ejecución.
	- Cada hilo se ejecutará en un núcleo CUDA
2. **Warp**
	- Son grupos de 32 hilos.
	- Unidad ejecutable más pequeña.
	- Todos sus hilos se ejecutan de manera sincronizada.
	- No todos los hilos de un warp ejecutan instrucciones.
3. **Bloque de hilo (Thread block)**
	- Unidad programable más pequeña.
		- Conjunto de hilos (max. 1024) que se define al lanzar un kernel.
		- Pueden trabajar en 1D, 2D y 3D. (Cuidado con el max. hilos)
	- Se asignan a un Streaming Multiprocessor (SM) para ejecutarse.
		- Cada SM tiene su memoria compartida.
		- Cada SM tiene un Warp Scheduler que organiza los Warps.
4. **Malla de bloques (Grid)**
	- Conjunto de bloques definido al lanzar el kernel.
	- CUDA balancea la carga entre los distintos SM.
	- Representa el problema total en la GPU.
5. GPU (device)
	- Se conoce como `device`
	- Ejecuta kernels en sus SM.
	- Tiene una memoria global para todos los SM.
